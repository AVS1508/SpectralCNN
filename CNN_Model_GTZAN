digraph {
	graph [size="25.349999999999998,25.349999999999998"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	132891306320544 [label="
 (16, 10)" fillcolor=darkolivegreen1]
	132891288763024 [label=AddmmBackward0]
	132891393565904 -> 132891288763024
	132891307457600 [label="dense2.bias
 (10)" fillcolor=lightblue]
	132891307457600 -> 132891393565904
	132891393565904 [label=AccumulateGrad]
	132891288760336 -> 132891288763024
	132891288760336 [label=ReluBackward0]
	132891288760480 -> 132891288760336
	132891288760480 [label=NativeBatchNormBackward0]
	132891393564560 -> 132891288760480
	132891393564560 [label=AddmmBackward0]
	132891393557744 -> 132891393564560
	132891307451760 [label="dense1.bias
 (64)" fillcolor=lightblue]
	132891307451760 -> 132891393557744
	132891393557744 [label=AccumulateGrad]
	132891393565472 -> 132891393564560
	132891393565472 [label=ViewBackward0]
	132891393559760 -> 132891393565472
	132891393559760 [label=MaxPool2DWithIndicesBackward0]
	132891393556976 -> 132891393559760
	132891393556976 [label=LeakyReluBackward0]
	132891393571712 -> 132891393556976
	132891393571712 [label=CudnnBatchNormBackward0]
	132891393557264 -> 132891393571712
	132891393557264 [label=ConvolutionBackward0]
	132891393572432 -> 132891393557264
	132891393572432 [label=MaxPool2DWithIndicesBackward0]
	132891393566192 -> 132891393572432
	132891393566192 [label=LeakyReluBackward0]
	132891393571760 -> 132891393566192
	132891393571760 [label=CudnnBatchNormBackward0]
	132891393570992 -> 132891393571760
	132891393570992 [label=ConvolutionBackward0]
	132891393561344 -> 132891393570992
	132891393561344 [label=MaxPool2DWithIndicesBackward0]
	132891393561872 -> 132891393561344
	132891393561872 [label=LeakyReluBackward0]
	132891393557456 -> 132891393561872
	132891393557456 [label=CudnnBatchNormBackward0]
	132891393569264 -> 132891393557456
	132891393569264 [label=ConvolutionBackward0]
	132891393563648 -> 132891393569264
	132891393563648 [label=MaxPool2DWithIndicesBackward0]
	132891393556880 -> 132891393563648
	132891393556880 [label=LeakyReluBackward0]
	132891393572096 -> 132891393556880
	132891393572096 [label=CudnnBatchNormBackward0]
	132891393557408 -> 132891393572096
	132891393557408 [label=ConvolutionBackward0]
	132891393569216 -> 132891393557408
	132891393569216 [label=MaxPool2DWithIndicesBackward0]
	132891393556544 -> 132891393569216
	132891393556544 [label=LeakyReluBackward0]
	132891393562736 -> 132891393556544
	132891393562736 [label=CudnnBatchNormBackward0]
	132891393569744 -> 132891393562736
	132891393569744 [label=ConvolutionBackward0]
	132891393572240 -> 132891393569744
	132891393572240 [label=CudnnBatchNormBackward0]
	132891393557216 -> 132891393572240
	132891313031264 [label="input_batch_norm.weight
 (1)" fillcolor=lightblue]
	132891313031264 -> 132891393557216
	132891393557216 [label=AccumulateGrad]
	132891393569120 -> 132891393572240
	132892520784464 [label="input_batch_norm.bias
 (1)" fillcolor=lightblue]
	132892520784464 -> 132891393569120
	132891393569120 [label=AccumulateGrad]
	132891393568304 -> 132891393569744
	132891473776368 [label="layer1.convolution.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	132891473776368 -> 132891393568304
	132891393568304 [label=AccumulateGrad]
	132891393571568 -> 132891393569744
	132891392549856 [label="layer1.convolution.bias
 (16)" fillcolor=lightblue]
	132891392549856 -> 132891393571568
	132891393571568 [label=AccumulateGrad]
	132891393567920 -> 132891393562736
	132891314527968 [label="layer1.batch_norm.weight
 (16)" fillcolor=lightblue]
	132891314527968 -> 132891393567920
	132891393567920 [label=AccumulateGrad]
	132891393561680 -> 132891393562736
	132891407592288 [label="layer1.batch_norm.bias
 (16)" fillcolor=lightblue]
	132891407592288 -> 132891393561680
	132891393561680 [label=AccumulateGrad]
	132891393568592 -> 132891393557408
	132891407585328 [label="layer2.convolution.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	132891407585328 -> 132891393568592
	132891393568592 [label=AccumulateGrad]
	132891393563264 -> 132891393557408
	132891407590608 [label="layer2.convolution.bias
 (16)" fillcolor=lightblue]
	132891407590608 -> 132891393563264
	132891393563264 [label=AccumulateGrad]
	132891393561824 -> 132891393572096
	132891407585168 [label="layer2.batch_norm.weight
 (16)" fillcolor=lightblue]
	132891407585168 -> 132891393561824
	132891393561824 [label=AccumulateGrad]
	132891393565280 -> 132891393572096
	132891407590928 [label="layer2.batch_norm.bias
 (16)" fillcolor=lightblue]
	132891407590928 -> 132891393565280
	132891393565280 [label=AccumulateGrad]
	132891393559088 -> 132891393569264
	132891407587808 [label="layer3.convolution.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
	132891407587808 -> 132891393559088
	132891393559088 [label=AccumulateGrad]
	132891393572384 -> 132891393569264
	132891407582848 [label="layer3.convolution.bias
 (32)" fillcolor=lightblue]
	132891407582848 -> 132891393572384
	132891393572384 [label=AccumulateGrad]
	132891393556832 -> 132891393557456
	132891407592928 [label="layer3.batch_norm.weight
 (32)" fillcolor=lightblue]
	132891407592928 -> 132891393556832
	132891393556832 [label=AccumulateGrad]
	132891393562784 -> 132891393557456
	132891407587728 [label="layer3.batch_norm.bias
 (32)" fillcolor=lightblue]
	132891407587728 -> 132891393562784
	132891393562784 [label=AccumulateGrad]
	132891393557936 -> 132891393570992
	132891402820304 [label="layer4.convolution.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	132891402820304 -> 132891393557936
	132891393557936 [label=AccumulateGrad]
	132891393567872 -> 132891393570992
	132891307454160 [label="layer4.convolution.bias
 (32)" fillcolor=lightblue]
	132891307454160 -> 132891393567872
	132891393567872 [label=AccumulateGrad]
	132891393569984 -> 132891393571760
	132891307454000 [label="layer4.batch_norm.weight
 (32)" fillcolor=lightblue]
	132891307454000 -> 132891393569984
	132891393569984 [label=AccumulateGrad]
	132891393567440 -> 132891393571760
	132891307443440 [label="layer4.batch_norm.bias
 (32)" fillcolor=lightblue]
	132891307443440 -> 132891393567440
	132891393567440 [label=AccumulateGrad]
	132891393559328 -> 132891393557264
	132891307445120 [label="layer5.convolution.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	132891307445120 -> 132891393559328
	132891393559328 [label=AccumulateGrad]
	132891393571520 -> 132891393557264
	132891307448000 [label="layer5.convolution.bias
 (64)" fillcolor=lightblue]
	132891307448000 -> 132891393571520
	132891393571520 [label=AccumulateGrad]
	132891393569792 -> 132891393571712
	132891307444880 [label="layer5.batch_norm.weight
 (64)" fillcolor=lightblue]
	132891307444880 -> 132891393569792
	132891393569792 [label=AccumulateGrad]
	132891393571904 -> 132891393571712
	132891307450480 [label="layer5.batch_norm.bias
 (64)" fillcolor=lightblue]
	132891307450480 -> 132891393571904
	132891393571904 [label=AccumulateGrad]
	132891393561056 -> 132891393564560
	132891393561056 [label=TBackward0]
	132891393569840 -> 132891393561056
	132891307442880 [label="dense1.weight
 (64, 64)" fillcolor=lightblue]
	132891307442880 -> 132891393569840
	132891393569840 [label=AccumulateGrad]
	132891393558032 -> 132891288760480
	132891304710672 [label="dense_bn.weight
 (64)" fillcolor=lightblue]
	132891304710672 -> 132891393558032
	132891393558032 [label=AccumulateGrad]
	132891393567584 -> 132891288760480
	132891304717152 [label="dense_bn.bias
 (64)" fillcolor=lightblue]
	132891304717152 -> 132891393567584
	132891393567584 [label=AccumulateGrad]
	132891288762256 -> 132891288763024
	132891288762256 [label=TBackward0]
	132891393566576 -> 132891288762256
	132891307455360 [label="dense2.weight
 (10, 64)" fillcolor=lightblue]
	132891307455360 -> 132891393566576
	132891393566576 [label=AccumulateGrad]
	132891288763024 -> 132891306320544
}
